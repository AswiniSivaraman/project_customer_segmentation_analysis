{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>order</th>\n",
       "      <th>country</th>\n",
       "      <th>session_id</th>\n",
       "      <th>page1_main_category</th>\n",
       "      <th>page2_clothing_model</th>\n",
       "      <th>colour</th>\n",
       "      <th>location</th>\n",
       "      <th>model_photography</th>\n",
       "      <th>price</th>\n",
       "      <th>price_2</th>\n",
       "      <th>page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "      <td>15648</td>\n",
       "      <td>3</td>\n",
       "      <td>C20</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>10018</td>\n",
       "      <td>2</td>\n",
       "      <td>B26</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>19388</td>\n",
       "      <td>3</td>\n",
       "      <td>C13</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>7181</td>\n",
       "      <td>2</td>\n",
       "      <td>B11</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>29</td>\n",
       "      <td>13493</td>\n",
       "      <td>2</td>\n",
       "      <td>B31</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day  order  country  session_id  page1_main_category  \\\n",
       "0  2008      6   22     21       29       15648                    3   \n",
       "1  2008      5   19      6       29       10018                    2   \n",
       "2  2008      7   15      2       29       19388                    3   \n",
       "3  2008      5    2      2       29        7181                    2   \n",
       "4  2008      6    9     16       29       13493                    2   \n",
       "\n",
       "  page2_clothing_model  colour  location  model_photography  price  price_2  \\\n",
       "0                  C20      13         1                  2     48        1   \n",
       "1                  B26      13         3                  1     57        1   \n",
       "2                  C13       9         5                  1     48        1   \n",
       "3                  B11       2         4                  1     43        2   \n",
       "4                  B31       9         5                  1     57        1   \n",
       "\n",
       "   page  \n",
       "0     2  \n",
       "1     2  \n",
       "2     1  \n",
       "3     1  \n",
       "4     2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = df.columns \n",
    "# for i in t:\n",
    "#     print(i)\n",
    "#     print(df[i].unique())\n",
    "#     print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# def detect_outliers_iqr(data, column):\n",
    "#     Q1 = data[column].quantile(0.25)\n",
    "#     Q3 = data[column].quantile(0.75)\n",
    "#     IQR = Q3 - Q1\n",
    "#     lower_bound = Q1 - 1.5 * IQR\n",
    "#     upper_bound = Q3 + 1.5 * IQR\n",
    "#     outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "#     return outliers, lower_bound, upper_bound\n",
    "\n",
    "# outlier_df = []\n",
    "# for col in numerical_cols:\n",
    "#     outliers,lower_bound,upper_bound = detect_outliers_iqr(df, col)\n",
    "#     outlier_df.append({'col' : col, 'outliers_count' : len(outliers), 'lower bound':lower_bound, 'upper bound': upper_bound,'min value' : df[col].min(), 'max_value' : df[col].max()})\n",
    "        \n",
    "# pd.DataFrame(outlier_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['country'].sort_values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# df['page2_clothing_model'] = le.fit_transform(df['page2_clothing_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['purchase_completed'] = df.apply(\n",
    "    lambda row: 1 if (row['page'] == 5 and row['price'] > 0 and row['order'] > 10) else 0, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(['year'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import ttest_ind\n",
    "# import pandas as pd\n",
    "\n",
    "# # List to store results\n",
    "# results = []\n",
    "\n",
    "# # Perform t-test for each feature\n",
    "# for col in df.columns:\n",
    "#     if col != 'purchase_completed':\n",
    "#         group1 = df[df['purchase_completed'] == 0][col]\n",
    "#         group2 = df[df['purchase_completed'] == 1][col]\n",
    "        \n",
    "#         # Perform t-test\n",
    "#         t_stat, p_value = ttest_ind(group1, group2, equal_var=False)\n",
    "        \n",
    "#         # Append results\n",
    "#         results.append((col, t_stat, p_value))\n",
    "\n",
    "# # Convert results to a DataFrame for easy interpretation\n",
    "# results_df = pd.DataFrame(results, columns=['Feature', 't_stat', 'p_value'])\n",
    "\n",
    "# # Display features with significant p-values (< 0.05)\n",
    "# significant_features = results_df[results_df['p_value'] < 0.05]\n",
    "# significant_features.sort_values(by='p_value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['month'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['purchase_completed'].value_counts()\n",
    "\n",
    "# undersample issue is there , solve this issue by performing oversampling using method like smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_weekend'] = df['day'].apply(lambda x: 1 if x in [6, 7] else 0)\n",
    "df['total_clicks'] = df.groupby('session_id')['order'].transform('count')\n",
    "df['max_page_reached'] = df.groupby('session_id')['page'].transform('max')\n",
    "df['season'] = df['month'].apply(lambda x: 'Summer' if x in [4, 5, 6] else 'Autumn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['page2_clothing_model', 'season', 'purchase_completed']\n",
    "cont_cols = ['month','day','order','country','session_id','page1_main_category','colour','location','model_photography','price','price_2','page','purchase_completed','is_weekend','total_clicks','max_page_reached']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['year',\n",
       " 'month',\n",
       " 'day',\n",
       " 'order',\n",
       " 'country',\n",
       " 'session_id',\n",
       " 'page1_main_category',\n",
       " 'colour',\n",
       " 'location',\n",
       " 'model_photography',\n",
       " 'price',\n",
       " 'price_2',\n",
       " 'page',\n",
       " 'purchase_completed',\n",
       " 'is_weekend',\n",
       " 'total_clicks',\n",
       " 'max_page_reached']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select_dtypes(include=['number']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>order</th>\n",
       "      <th>country</th>\n",
       "      <th>session_id</th>\n",
       "      <th>page1_main_category</th>\n",
       "      <th>page2_clothing_model</th>\n",
       "      <th>colour</th>\n",
       "      <th>location</th>\n",
       "      <th>model_photography</th>\n",
       "      <th>price</th>\n",
       "      <th>price_2</th>\n",
       "      <th>page</th>\n",
       "      <th>purchase_completed</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>total_clicks</th>\n",
       "      <th>max_page_reached</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "      <td>15648</td>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>4</td>\n",
       "      <td>Summer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>10018</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>Summer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>19388</td>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>Autumn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>7181</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>Summer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>29</td>\n",
       "      <td>13493</td>\n",
       "      <td>2</td>\n",
       "      <td>66</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>Summer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   month  day  order  country  session_id  page1_main_category  \\\n",
       "0      6   22     21       29       15648                    3   \n",
       "1      5   19      6       29       10018                    2   \n",
       "2      7   15      2       29       19388                    3   \n",
       "3      5    2      2       29        7181                    2   \n",
       "4      6    9     16       29       13493                    2   \n",
       "\n",
       "   page2_clothing_model  colour  location  model_photography  price  price_2  \\\n",
       "0                    88      13         1                  2     48        1   \n",
       "1                    60      13         3                  1     57        1   \n",
       "2                    80       9         5                  1     48        1   \n",
       "3                    45       2         4                  1     43        2   \n",
       "4                    66       9         5                  1     57        1   \n",
       "\n",
       "   page  purchase_completed  is_weekend  total_clicks  max_page_reached  \\\n",
       "0     2                   0           0            84                 4   \n",
       "1     2                   0           0             9                 2   \n",
       "2     1                   0           0            10                 3   \n",
       "3     1                   0           0             6                 2   \n",
       "4     2                   0           0            15                 2   \n",
       "\n",
       "   season  \n",
       "0  Summer  \n",
       "1  Summer  \n",
       "2  Autumn  \n",
       "3  Summer  \n",
       "4  Summer  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import SMOTE\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import classification_report\n",
    "# import pandas as pd\n",
    "\n",
    "# # Step 1: Load train and test datasets\n",
    "# train_df = pd.read_csv(\"train_data.csv\")\n",
    "# test_df = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "# # Step 2: Separate features and target variable\n",
    "# X_train = train_df.drop('purchase_completed', axis=1)\n",
    "# y_train = train_df['purchase_completed']\n",
    "# X_test = test_df.drop('purchase_completed', axis=1)\n",
    "# y_test = test_df['purchase_completed']\n",
    "\n",
    "# # Step 3: Apply SMOTE to balance the training dataset\n",
    "# smote = SMOTE(random_state=42)\n",
    "# X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# # Step 4: Train a RandomForestClassifier on the SMOTE-balanced training set\n",
    "# model = RandomForestClassifier(random_state=42)\n",
    "# model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# # Step 5: Evaluate the model on the original test set\n",
    "# y_pred = model.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import logging\n",
    "# from scipy.stats import chi2_contingency,ttest_ind\n",
    "# from scipy.stats import f_oneway, pearsonr\n",
    "\n",
    "# def Feature_Selection_Classification(df:pd.DataFrame, \n",
    "#                                      continuous_cols:list,\n",
    "#                                      categorical_cols:list, \n",
    "#                                      target_col:str) -> list:\n",
    "#     \"\"\"\n",
    "    \n",
    "\n",
    "#     \"\"\"\n",
    "\n",
    "#     try:\n",
    "\n",
    "#         logging.info(\"Feature Selection for classification Problem\")\n",
    "\n",
    "#         # Remove target column from feature lists\n",
    "#         if target_col in continuous_cols:\n",
    "#             continuous_cols.remove(target_col)\n",
    "#         if target_col in categorical_cols:\n",
    "#             categorical_cols.remove(target_col)\n",
    "\n",
    "#         results = []\n",
    "\n",
    "#         # Perform T-Test for continuous variables\n",
    "#         for col in continuous_cols:\n",
    "#             group1 = df[df[target_col] == df[target_col].unique()[0]][col]\n",
    "#             group2 = df[df[target_col] == df[target_col].unique()[1]][col]\n",
    "            \n",
    "#             stat, p_value = ttest_ind(group1, group2, equal_var=False)  # T-Test\n",
    "#             results.append({\"Feature\": col, \"Test\": \"T-Test\", \"P-Value\": p_value, \"Significant (<0.05)\": p_value < 0.05})\n",
    "\n",
    "#         # Perform Chi-Square Test for categorical variables\n",
    "#         for col in categorical_cols:\n",
    "#             contingency_table = pd.crosstab(df[col], df[target_col])\n",
    "#             chi2_stat, p_value, _, _ = chi2_contingency(contingency_table)\n",
    "#             results.append({\"Feature\": col, \"Test\": \"Chi-Square\", \"P-Value\": p_value, \"Significant (<0.05)\": p_value < 0.05})\n",
    "\n",
    "#         # Convert to DataFrame and display results\n",
    "#         results_df = pd.DataFrame(results)\n",
    "\n",
    "#         # Filter the results to get only the features with significance < 0.05\n",
    "#         significant_features1 = results_df[results_df[\"Significant (<0.05)\"] == True][\"Feature\"].tolist()\n",
    "\n",
    "#         return significant_features1\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         logging.error('Error in Feature Selection for Classification Problem')\n",
    "#         raise e\n",
    "\n",
    "\n",
    "# def Feature_Selection_Regression(df:pd.DataFrame, \n",
    "#                                      continuous_cols:list,\n",
    "#                                      categorical_cols:list, \n",
    "#                                      target_col:str) -> list :\n",
    "    \n",
    "#     \"\"\"\n",
    "    \n",
    "    \n",
    "#     \"\"\"\n",
    "    \n",
    "#     try:\n",
    "\n",
    "#         logging.info('Feature Selection for Regression Problem')\n",
    "\n",
    "#         # Identify continuous and categorical columns\n",
    "#         continuous_cols = ['month', 'day', 'order', 'country', 'session_id', 'page1_main_category',\n",
    "#                     'colour', 'location', 'model_photography',\n",
    "#                     'price', 'price_2', 'page','is_weekend', 'total_clicks',\n",
    "#                     'max_page_reached']\n",
    "\n",
    "#         categorical_cols = ['page2_clothing_model','target']\n",
    "\n",
    "#         target_col = 'price'  # Replace with your actual target column\n",
    "\n",
    "#         # Remove target column from feature lists\n",
    "#         if target_col in continuous_cols:\n",
    "#             continuous_cols.remove(target_col)\n",
    "#         if target_col in categorical_cols:\n",
    "#             categorical_cols.remove(target_col)\n",
    "\n",
    "#         results = []\n",
    "\n",
    "#         # Perform ANOVA F-Test for categorical variables\n",
    "#         for col in categorical_cols:\n",
    "#             groups = [df[df[col] == cat][target_col] for cat in df[col].unique()]\n",
    "#             f_stat, p_value = f_oneway(*groups)\n",
    "#             results.append({\"Feature\": col, \"Test\": \"ANOVA\", \"P-Value\": p_value, \"Significant (<0.05)\": p_value < 0.05})\n",
    "\n",
    "#         # Perform Pearson Correlation Test for continuous variables\n",
    "#         for col in continuous_cols:\n",
    "#             corr_coeff, p_value = pearsonr(df[col], df[target_col])\n",
    "#             results.append({\"Feature\": col, \"Test\": \"Pearson Correlation\", \"P-Value\": p_value, \"Significant (<0.05)\": p_value < 0.05})\n",
    "\n",
    "#         # Convert results to DataFrame\n",
    "#         results_df = pd.DataFrame(results)\n",
    "\n",
    "#         # Extract only significant features\n",
    "#         significant_features2 = results_df[results_df[\"Significant (<0.05)\"] == True][\"Feature\"].tolist()\n",
    "\n",
    "#         return significant_features2\n",
    "\n",
    "#     except Exception as e:\n",
    "#         logging.error('Error in Feature Selection for Regression Problem')\n",
    "#         raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Automated Hyperparameter Tuning for the Best Model\n",
    "# To automate hyperparameter tuning for the best model, we will:\n",
    "\n",
    "# Select the best model from classification, regression, or clustering.\n",
    "# Perform hyperparameter tuning using Optuna (for efficient optimization).\n",
    "# Retrain the model with the optimized hyperparameters.\n",
    "# Evaluate the tuned model to ensure improvement.\n",
    "# Log all results in MLflow for tracking.\n",
    "\n",
    "# pip install optuna mlflow zenml[mlflow]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# import logging\n",
    "# import pickle\n",
    "# import mlflow\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "# from xgboost import XGBClassifier, XGBRegressor\n",
    "# from sklearn.cluster import KMeans\n",
    "# from zenml.client import Client\n",
    "\n",
    "# # âœ… Get MLflow Tracker\n",
    "# experiment_tracker = Client().active_stack.experiment_tracker\n",
    "\n",
    "\n",
    "# def tune_classification_model(X_train, y_train, best_model_name):\n",
    "#     \"\"\"\n",
    "#     Tune the best classification model using Optuna.\n",
    "#     \"\"\"\n",
    "#     def objective(trial):\n",
    "#         if best_model_name == \"RandomForestClassifier\":\n",
    "#             params = {\n",
    "#                 \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 200),\n",
    "#                 \"max_depth\": trial.suggest_int(\"max_depth\", 3, 20),\n",
    "#             }\n",
    "#             model = RandomForestClassifier(**params, random_state=42)\n",
    "\n",
    "#         elif best_model_name == \"XGBClassifier\":\n",
    "#             params = {\n",
    "#                 \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 200),\n",
    "#                 \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "#             }\n",
    "#             model = XGBClassifier(**params, random_state=42)\n",
    "\n",
    "#         else:\n",
    "#             logging.warning(\"âš  No tuning available for this model\")\n",
    "#             return 0\n",
    "\n",
    "#         return cross_val_score(model, X_train, y_train, cv=3, scoring=\"accuracy\").mean()\n",
    "\n",
    "#     study = optuna.create_study(direction=\"maximize\")\n",
    "#     study.optimize(objective, n_trials=10)\n",
    "\n",
    "#     best_params = study.best_params\n",
    "#     logging.info(f\"âœ… Best Hyperparameters: {best_params}\")\n",
    "\n",
    "#     # Train final model\n",
    "#     if best_model_name == \"RandomForestClassifier\":\n",
    "#         best_model = RandomForestClassifier(**best_params, random_state=42)\n",
    "#     else:\n",
    "#         best_model = XGBClassifier(**best_params, random_state=42)\n",
    "\n",
    "#     best_model.fit(X_train, y_train)\n",
    "\n",
    "#     # Save tuned model\n",
    "#     model_path = f\"models/tuned_{best_model_name}.pkl\"\n",
    "#     with open(model_path, \"wb\") as f:\n",
    "#         pickle.dump(best_model, f)\n",
    "\n",
    "#     # Log to MLflow\n",
    "#     if experiment_tracker:\n",
    "#         mlflow.log_params(best_params)\n",
    "\n",
    "#     return best_model, model_path\n",
    "\n",
    "\n",
    "# def tune_regression_model(X_train, y_train, best_model_name):\n",
    "#     \"\"\"\n",
    "#     Tune the best regression model using Optuna.\n",
    "#     \"\"\"\n",
    "#     def objective(trial):\n",
    "#         if best_model_name == \"RandomForestRegressor\":\n",
    "#             params = {\n",
    "#                 \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 200),\n",
    "#                 \"max_depth\": trial.suggest_int(\"max_depth\", 3, 20),\n",
    "#             }\n",
    "#             model = RandomForestRegressor(**params, random_state=42)\n",
    "\n",
    "#         elif best_model_name == \"XGBRegressor\":\n",
    "#             params = {\n",
    "#                 \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 200),\n",
    "#                 \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "#             }\n",
    "#             model = XGBRegressor(**params, random_state=42)\n",
    "\n",
    "#         else:\n",
    "#             logging.warning(\"âš  No tuning available for this model\")\n",
    "#             return float(\"inf\")\n",
    "\n",
    "#         return -cross_val_score(model, X_train, y_train, cv=3, scoring=\"neg_mean_squared_error\").mean()\n",
    "\n",
    "#     study = optuna.create_study(direction=\"minimize\")\n",
    "#     study.optimize(objective, n_trials=10)\n",
    "\n",
    "#     best_params = study.best_params\n",
    "#     logging.info(f\"âœ… Best Hyperparameters: {best_params}\")\n",
    "\n",
    "#     if best_model_name == \"RandomForestRegressor\":\n",
    "#         best_model = RandomForestRegressor(**best_params, random_state=42)\n",
    "#     else:\n",
    "#         best_model = XGBRegressor(**best_params, random_state=42)\n",
    "\n",
    "#     best_model.fit(X_train, y_train)\n",
    "\n",
    "#     # Save tuned model\n",
    "#     model_path = f\"models/tuned_{best_model_name}.pkl\"\n",
    "#     with open(model_path, \"wb\") as f:\n",
    "#         pickle.dump(best_model, f)\n",
    "\n",
    "#     if experiment_tracker:\n",
    "#         mlflow.log_params(best_params)\n",
    "\n",
    "#     return best_model, model_path\n",
    "\n",
    "\n",
    "# def tune_clustering_model(X_train, best_model_name):\n",
    "#     \"\"\"\n",
    "#     Tune the best clustering model using Optuna.\n",
    "#     \"\"\"\n",
    "#     def objective(trial):\n",
    "#         if best_model_name == \"KMeans\":\n",
    "#             params = {\n",
    "#                 \"n_clusters\": trial.suggest_int(\"n_clusters\", 2, 10),\n",
    "#             }\n",
    "#             model = KMeans(**params, random_state=42)\n",
    "#         else:\n",
    "#             logging.warning(\"âš  No tuning available for this model\")\n",
    "#             return 0\n",
    "\n",
    "#         model.fit(X_train)\n",
    "#         return -model.inertia_\n",
    "\n",
    "#     study = optuna.create_study(direction=\"minimize\")\n",
    "#     study.optimize(objective, n_trials=10)\n",
    "\n",
    "#     best_params = study.best_params\n",
    "#     logging.info(f\"âœ… Best Hyperparameters: {best_params}\")\n",
    "\n",
    "#     best_model = KMeans(**best_params, random_state=42)\n",
    "#     best_model.fit(X_train)\n",
    "\n",
    "#     model_path = f\"models/tuned_{best_model_name}.pkl\"\n",
    "#     with open(model_path, \"wb\") as f:\n",
    "#         pickle.dump(best_model, f)\n",
    "\n",
    "#     if experiment_tracker:\n",
    "#         mlflow.log_params(best_params)\n",
    "\n",
    "#     return best_model, model_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# from pipelines.classification_pipeline import classification_pipeline\n",
    "# from pipelines.regression_pipeline import regression_pipeline\n",
    "# from pipelines.clustering_pipeline import clustering_pipeline\n",
    "# from utils.hyperparameter_tuning import tune_classification_model, tune_regression_model, tune_clustering_model\n",
    "# from zenml.client import Client\n",
    "\n",
    "# def run_main_pipeline(train_data_path: str, test_data_path: str):\n",
    "#     \"\"\"\n",
    "#     Runs classification, regression, and clustering pipelines sequentially with hyperparameter tuning.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # âœ… Initialize MLflow Experiment Tracking\n",
    "#     experiment_tracker = Client().active_stack.experiment_tracker\n",
    "#     if experiment_tracker:\n",
    "#         logging.info(f\"ðŸ” Using MLflow experiment tracker: {experiment_tracker.name}\")\n",
    "\n",
    "#     logging.info(\"ðŸš€ Starting Classification Pipeline...\")\n",
    "#     best_classification_model, _ = classification_pipeline(train_data_path, test_data_path)\n",
    "\n",
    "#     logging.info(\"ðŸš€ Starting Regression Pipeline...\")\n",
    "#     best_regression_model, _ = regression_pipeline(train_data_path, test_data_path)\n",
    "\n",
    "#     logging.info(\"ðŸš€ Starting Clustering Pipeline...\")\n",
    "#     best_clustering_model, _ = clustering_pipeline(train_data_path, test_data_path)\n",
    "\n",
    "#     logging.info(f\"ðŸ† Best Classification Model: {best_classification_model}\")\n",
    "#     logging.info(f\"ðŸ† Best Regression Model: {best_regression_model}\")\n",
    "#     logging.info(f\"ðŸ† Best Clustering Model: {best_clustering_model}\")\n",
    "\n",
    "#     # âœ… Hyperparameter Tuning\n",
    "#     logging.info(\"ðŸš€ Starting Hyperparameter Tuning...\")\n",
    "#     tune_classification_model(best_classification_model, train_data_path)\n",
    "#     tune_regression_model(best_regression_model, train_data_path)\n",
    "#     tune_clustering_model(best_clustering_model, train_data_path)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     train_file_path = \"data/train_data.csv\"\n",
    "#     test_file_path = \"data/test_data.csv\"\n",
    "#     run_main_pipeline(train_file_path, test_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guvi_projects_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
